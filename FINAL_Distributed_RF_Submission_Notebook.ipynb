{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distributed Random Forest using Apache Spark\n",
        "## Programming Assignment \u2013 Parallel Machine Learning\n",
        "\n",
        "This notebook trains a Distributed Random Forest using Apache Spark,\n",
        "evaluates performance, and measures speedup.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install pyspark if needed\n",
        "# !pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName('DistributedRF') \\\n",
        "    .master('local[*]') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel('ERROR')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "Make sure `distributed_rf_dataset.csv` is in the same folder as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_path = 'distributed_rf_dataset.csv'\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "label_column = 'label'\n",
        "feature_cols = [c for c in df.columns if c != label_column]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "df = assembler.transform(df).select('features', label_column)\n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_model(partitions):\n",
        "    temp_df = df.repartition(partitions)\n",
        "    train_data, test_data = temp_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        featuresCol='features',\n",
        "        labelCol=label_column,\n",
        "        numTrees=100,\n",
        "        maxDepth=10\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model = rf.fit(train_data)\n",
        "    training_time = time.time() - start\n",
        "\n",
        "    predictions = model.transform(test_data)\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=label_column)\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "\n",
        "    return training_time, auc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "partitions_list = [1, 2, 4, 8]\n",
        "\n",
        "results = []\n",
        "\n",
        "for p in partitions_list:\n",
        "    print(f'Training with {p} partitions...')\n",
        "    t, auc = train_model(p)\n",
        "    results.append((p, t, auc))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['Nodes', 'Training_Time', 'AUC'])\n",
        "\n",
        "# Compute Speedup\n",
        "baseline_time = results_df.loc[results_df['Nodes'] == 1, 'Training_Time'].values[0]\n",
        "results_df['Speedup'] = baseline_time / results_df['Training_Time']\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot 1: Training Time vs Nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "plt.plot(results_df['Nodes'], results_df['Training_Time'], marker='o')\n",
        "plt.xlabel('Number of Nodes')\n",
        "plt.ylabel('Training Time (seconds)')\n",
        "plt.title('Training Time vs Number of Nodes')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot 2: Speedup vs Nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "plt.plot(results_df['Nodes'], results_df['Speedup'], marker='o')\n",
        "plt.xlabel('Number of Nodes')\n",
        "plt.ylabel('Speedup')\n",
        "plt.title('Speedup vs Number of Nodes')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results_df.to_csv('experiment_results.csv', index=False)\n",
        "results_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}